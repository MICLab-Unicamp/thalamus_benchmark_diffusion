{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-12T19:35:19.841537Z",
     "start_time": "2021-05-12T19:35:19.824582Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch Lightning Version: 1.3.8\n",
      "Torch Version: 1.8.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import argparse\n",
    "from glob import glob\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "from CNNs.unet import UNet\n",
    "from Utils.transforms import My_transforms\n",
    "import Utils.datasets as my_datasets\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "from Utils.loss import SoftDiceLoss\n",
    "from Utils.loss import DiceLoss\n",
    "from Utils.loss import DiceLoss_chavg\n",
    "from Utils.loss import DiceLoss_weighs\n",
    "from Utils.loss import CombinedLoss\n",
    "from Utils.Metrics import DiceMetric_weighs\n",
    "\n",
    "import time\n",
    "import multiprocessing as mp\n",
    "\n",
    "%load_ext tensorboard\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
    "# from pytorch_lightning.loggers import NeptuneLogger\n",
    "\n",
    "print(f\"Pytorch Lightning Version: {pl.__version__}\")\n",
    "print(f\"Torch Version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 - Definition of traning and model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-12T19:36:23.711868Z",
     "start_time": "2021-05-12T19:36:23.697894Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'experiment_name': 'unet_single_label',\n",
       " 'description': 'unet_single_label_psz064_all_evalue1T1',\n",
       " 'data_view': 'all',\n",
       " 'dataset_folder': './Data/Patches/single_label_psz064/',\n",
       " 'subjects_list': './training_subjects_randomized.txt',\n",
       " 'in_channels': ['evalue1', 'T1'],\n",
       " 'masks': ['staple_oh'],\n",
       " 'cnn_architecture': 'unet',\n",
       " 'input_size': 32,\n",
       " 'n_inchannels': 2,\n",
       " 'n_outchannels': 2,\n",
       " 'init_features': 32,\n",
       " 'taug_angle': 5,\n",
       " 'taug_flip_prob': 0.5,\n",
       " 'max_epochs': 2000,\n",
       " 'patience': 20,\n",
       " 'learning_threshold': 0.01,\n",
       " 'batch_size': 256,\n",
       " 'split_train_val': 0.2,\n",
       " 'opt_name': 'Adam',\n",
       " 'min_lr': 1e-06,\n",
       " 'eps': 1e-05,\n",
       " 'monitor': 'val_loss',\n",
       " 'lr': 0.001,\n",
       " 'scheduling_patience_lrepochs': 3,\n",
       " 'lr_decay_factor': 0.1,\n",
       " 'lr_decay_policy': 'plateau',\n",
       " 'lr_decay_mode': 'min',\n",
       " 'lr_decay_threshold_mode': 'abs',\n",
       " 'lossweighs': [[0, 1]],\n",
       " 'func_weights': [1],\n",
       " 'train_loss_funcs': ['DiceLoss_weighs'],\n",
       " 'train_metric': 'DiceMetric_weighs',\n",
       " 'train_metricweighs': [0, 1],\n",
       " 'val_loss_funcs': ['DiceLoss_weighs'],\n",
       " 'val_metric': 'DiceMetric_weighs',\n",
       " 'val_metricweighs': [0, 1],\n",
       " 'nworkers': 8,\n",
       " 'val_transform': {}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Definitions of map combination\n",
    "\n",
    "# input_channels=['evalue1', 'FA', 'RD', 'MD', 'T1']\n",
    "# input_channels=['evalue1', 'FA', 'RD', 'MD']\n",
    "# input_channels=['T1']\n",
    "# input_channels=['FA', 'RD']\n",
    "# input_channels=['FA', 'T1']\n",
    "# input_channels=['FA', 'MD']\n",
    "# input_channels=['FA', 'evalue1']\n",
    "# input_channels=['RD', 'MD']\n",
    "# input_channels=['FA']\n",
    "# input_channels=['MD']\n",
    "# input_channels=['RD']\n",
    "# input_channels=['evalue1']\n",
    "# input_channels=['MD', 'T1']\n",
    "input_channels=['evalue1', 'T1']\n",
    "\n",
    "output_channels=['staple_oh']\n",
    "\n",
    "\n",
    "w = 1\n",
    "lossweighs = [[0, w]]\n",
    "\n",
    "loss_funcs = ['DiceLoss_weighs']\n",
    "func_weights = [1]\n",
    "\n",
    "metricweight = [0,1]\n",
    "train_metric = 'DiceMetric_weighs'\n",
    "val_metric = 'DiceMetric_weighs'\n",
    "\n",
    "view = \"all\" # TRAINS ALL DATA VIEWS AUTOMATICALLY\n",
    "# view = \"sagittal\"\n",
    "# view = \"coronal\"\n",
    "# view = \"axial\"\n",
    "\n",
    "hyperparameters = {\"experiment_name\": \"unet_single_label\", #experiment params\n",
    "                   \"description\": f\"unet_single_label_psz064_{view}_{''.join(input_channels)}\",\n",
    "                   \"data_view\": view,  \n",
    "                   \"dataset_folder\": './Data/Patches/single_label_psz064/',\n",
    "                   \"subjects_list\": './training_subjects_randomized.txt',\n",
    "           \n",
    "                   \"in_channels\":input_channels,\n",
    "                   \"masks\":output_channels,\n",
    "# CNN architecture\n",
    "                   \"cnn_architecture\": 'unet',\n",
    "                   \"input_size\": 32,\n",
    "                   \"n_inchannels\": len(input_channels),\n",
    "                   \"n_outchannels\": len(output_channels)*2,\n",
    "                   \"init_features\": 32,              \n",
    "#augentation                                     \n",
    "                   \"taug_angle\": 5,\n",
    "                   \"taug_flip_prob\": 0.5,       \n",
    "# train_params                 \n",
    "                   \"max_epochs\": 2000,\n",
    "                   \"patience\": 20,  # patience for early stop\n",
    "                   \"learning_threshold\": 0.01, # default = 0.01\n",
    "                   \"batch_size\": 256,  # Bigger is faster. limited by RAM or VRAM\n",
    "#                    \"batch_size\": 512,\n",
    "#                    \"batch_size\": 128,\n",
    "#                    \"batch_size\": 64, \n",
    "#                    \"batch_size\": 16, \n",
    "                   \"split_train_val\": 0.2, \n",
    "                   \n",
    "                   \"opt_name\": \"Adam\", \n",
    "#                    \"opt_name\": \"SGD\", \n",
    "                   \"min_lr\": 1e-06,\n",
    "                   \"eps\": 1e-05,\n",
    "                   \"monitor\": 'val_loss',\n",
    "                   \"lr\": 1e-3,  \n",
    "#                    \"lr\": 1e-5,  \n",
    "                   \"scheduling_patience_lrepochs\": 3, # patience for lr deacay\n",
    "                   \"lr_decay_factor\": 0.1, \n",
    "                   \"lr_decay_policy\": 'plateau', # plateau or step\n",
    "                   \"lr_decay_mode\": 'min',\n",
    "                   \"lr_decay_threshold_mode\" : 'abs', #rel, abs (plateau only)\n",
    "                   \n",
    "                   \n",
    "                   \"lossweighs\": lossweighs,\n",
    "                   \"func_weights\": func_weights,\n",
    "                   \"train_loss_funcs\": loss_funcs,\n",
    "                   \"train_metric\": train_metric,\n",
    "                   \"train_metricweighs\": metricweight,\n",
    "                   \"val_loss_funcs\": loss_funcs,\n",
    "                   \"val_metric\": val_metric,\n",
    "                   \"val_metricweighs\": metricweight,\n",
    "                   \"nworkers\": 8,\n",
    "                   \"val_transform\": {},\n",
    "                   \n",
    "                   \n",
    "                  }\n",
    "original_hyperparameters = hyperparameters.copy()\n",
    "hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 - Defining modules and classes (PyTorch Lightning Framework)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-12T19:36:26.036100Z",
     "start_time": "2021-05-12T19:36:25.999170Z"
    }
   },
   "outputs": [],
   "source": [
    "class MyDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__()\n",
    "        self.hparams = hparams\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        pass\n",
    "\n",
    "    def train_dataloader(self):   \n",
    "        train_dataset = my_datasets.PatchDataSet(subject_list=self.hparams['subjects_list'], root=self.hparams['dataset_folder'] + self.hparams['data_view'] + '/',\n",
    "                                         channels=self.hparams['in_channels'],\n",
    "                                         masks=self.hparams['masks'],\n",
    "                                         transform=self.train_transforms,\n",
    "                                         valid_split=self.hparams['split_train_val']\n",
    "                                         )        \n",
    "        train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=self.hparams['batch_size'],\n",
    "                                           shuffle=True,\n",
    "                                           num_workers=self.hparams['nworkers'], pin_memory=False)       \n",
    "        \n",
    "        return train_loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        val_dataset = my_datasets.PatchDataSet(subject_list=self.hparams['subjects_list'], root=self.hparams['dataset_folder'] + self.hparams['data_view'] + '/',\n",
    "                                         channels=self.hparams['in_channels'],\n",
    "                                         masks=self.hparams['masks'],\n",
    "                                         transform=self.val_transforms,\n",
    "                                         valid_split=self.hparams['split_train_val'],\n",
    "                                         validation=True\n",
    "                                         )        \n",
    "        val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                           batch_size=self.hparams['batch_size'],\n",
    "                                           shuffle=False,\n",
    "                                           num_workers=self.hparams['nworkers'], pin_memory=False)   \n",
    "        return val_loader\n",
    "\n",
    "class Segmentor(pl.LightningModule):\n",
    "    def __init__(self, hparams: argparse.Namespace):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters(hparams)        \n",
    "\n",
    "        if \"unet\" in self.hparams.cnn_architecture:\n",
    "            architecture = UNet(nin_channels=self.hparams.n_inchannels, \n",
    "                                nout_channels=self.hparams.n_outchannels, \n",
    "                                init_features=self.hparams.init_features)\n",
    "        elif self.hparams.cnn_architecture == \"coedet\":\n",
    "            architecture = CoEDET(nin=self.hparams.n_inchannels, nout=self.hparams.n_outchannels, \n",
    "                                  apply_sigmoid=self.hparams.apply_sigmoid)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported cnn_architecture {self.hparams.cnn_architecture}\")\n",
    "\n",
    "        self.model = architecture\n",
    "    \n",
    "        \n",
    "        ttransform_scale=None\n",
    "        ttransform_angle=None\n",
    "        ttransform_flip_prob=None\n",
    "        ttransform_sigma=None\n",
    "        ttransform_ens_treshold=None\n",
    "        if \"taug_scale\" in self.hparams:\n",
    "            ttransform_scale = self.hparams.taug_scale\n",
    "        if \"taug_angle\" in self.hparams:\n",
    "            ttransform_angle = self.hparams.taug_angle\n",
    "        if \"taug_flip_prob\" in self.hparams:\n",
    "            ttransform_flip_prob = self.hparams.taug_flip_prob\n",
    "        if \"taug_sigma\" in self.hparams:\n",
    "            ttransform_sigma = self.hparams.taug_sigma\n",
    "        if \"taug_ens_treshold\" in self.hparams:\n",
    "            ttransform_ens_treshold = self.hparams.aug_ens_treshold\n",
    "        self.train_transforms = My_transforms(scale=ttransform_scale,\n",
    "                                         angle=ttransform_angle,\n",
    "                                         flip_prob=ttransform_flip_prob,\n",
    "                                         sigma=ttransform_sigma,\n",
    "                                         ens_treshold=ttransform_ens_treshold\n",
    "                                        )\n",
    "        vtransform_scale=None\n",
    "        vtransform_angle=None\n",
    "        vtransform_flip_prob=None\n",
    "        vtransform_sigma=None\n",
    "        vtransform_ens_treshold=None\n",
    "        if \"vaug_scale\" in self.hparams:\n",
    "            ttransform_scale = self.hparams.vaug_scale\n",
    "        if \"vaug_angle\" in self.hparams:\n",
    "            ttransform_angle = self.hparams.vaug_angle\n",
    "        if \"vaug_flip_prob\" in self.hparams:\n",
    "            ttransform_flip_prob = self.hparams.vaug_flip_prob\n",
    "        if \"vaug_sigma\" in self.hparams:\n",
    "            ttransform_sigma = self.hparams.vaug_sigma\n",
    "        if \"vaug_ens_treshold\" in self.hparams:\n",
    "            ttransform_ens_treshold = self.hparams.vaug_ens_treshold\n",
    "        self.val_transforms = My_transforms(scale=vtransform_scale,\n",
    "                                         angle=vtransform_angle,\n",
    "                                         flip_prob=vtransform_flip_prob,\n",
    "                                         sigma=vtransform_sigma,\n",
    "                                         ens_treshold=vtransform_ens_treshold\n",
    "                                        )\n",
    "        \n",
    "  \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        loss = None\n",
    "\n",
    "        x, y = train_batch\n",
    "        logits = self.forward(x)\n",
    "#         print('x.shape = ', x.shape)\n",
    "#         print('y.shape = ', y.shape)\n",
    "#         print('logits.shape = ', logits.shape)\n",
    "\n",
    "        loss = CombinedLoss(logits, y, \n",
    "                            self.hparams.train_loss_funcs, \n",
    "                            self.hparams.lossweighs,\n",
    "                            func_weights=self.hparams.func_weights)\n",
    "\n",
    "        if self.hparams.train_metric == 'DiceMetric_weighs':\n",
    "            train_metric = DiceMetric_weighs(y_pred=logits, y_true=y,\n",
    "                                             weights=self.hparams.train_metricweighs, treshold=0.5)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported metric {self.hparams.train_metric}\")\n",
    "\n",
    "        self.log(\"loss\", loss, on_epoch=True, on_step=True)\n",
    "        self.log(\"train_metric\", train_metric, on_epoch=True, on_step=False)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        logits = None\n",
    "\n",
    "        x, y = val_batch\n",
    "        logits = self.forward(x)\n",
    "#         loss = self.lossfunc(logits, y)\n",
    "\n",
    "        loss = CombinedLoss(logits, y, \n",
    "                            self.hparams.val_loss_funcs, \n",
    "                            self.hparams.lossweighs,\n",
    "                            func_weights=self.hparams.func_weights)\n",
    "    \n",
    "        if self.hparams.val_metric == 'DiceMetric_weighs':\n",
    "            val_metric = DiceMetric_weighs(y_pred=logits, y_true=y,\n",
    "                                             weights=self.hparams.val_metricweighs, treshold=0.5)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported metric {self.hparams.val_metric}\")\n",
    "\n",
    "        self.log(\"val_loss\", loss, on_epoch=True, on_step=False, prog_bar=True)\n",
    "        self.log(\"val_metric\", val_metric, on_epoch=True, on_step=False, prog_bar=True)        \n",
    "        self.log(\"learning_rate_test\", self.optimizer.param_groups[0]['lr'], on_epoch=True, on_step=False, prog_bar=False)\n",
    "\n",
    "    def get_optimizer_by_name(self, name, lr):\n",
    "        if name == \"Adam\":\n",
    "            return Adam(self.model.parameters(), lr=lr)\n",
    "        elif name == \"SGD\":\n",
    "            return SGD(self.model.parameters(), lr=lr)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported optimizer: {name}\")\n",
    "            \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        '''\n",
    "        Select optimizer and scheduling strategy according to hparams.\n",
    "        '''\n",
    "        optimizer = self.get_optimizer_by_name(self.hparams.opt_name, \n",
    "                                               self.hparams.lr)\n",
    "\n",
    "        if self.hparams.lr_decay_policy == 'step':\n",
    "            scheduler = StepLR(optimizer, self.hparams.scheduling_patience_lrepochs, self.hparams.lr_decay_factor, verbose=True)\n",
    "            print('STEP - scheduling_patience_lrepochs = ', self.hparams.scheduling_patience_lrepochs, ' lr_decay_factor = ', self.hparams.lr_decay_factor)\n",
    "        elif self.hparams.lr_decay_policy == 'plateau':\n",
    "            print('PLATEAU - scheduling_patience_lrepochs = ', self.hparams.scheduling_patience_lrepochs, ' lr_decay_factor = ', self.hparams.lr_decay_factor)\n",
    "            \n",
    "            self.optimizer = optimizer\n",
    "            lr_scheduler =  {\n",
    "                           'scheduler': ReduceLROnPlateau(optimizer),\n",
    "                           'mode': self.hparams.lr_decay_mode,\n",
    "                           'factor': self.hparams.lr_decay_factor,\n",
    "                           'patience': self.hparams.scheduling_patience_lrepochs,\n",
    "                           'threshold': self.hparams.learning_threshold,\n",
    "                           'threshold_mode': self.hparams.lr_decay_threshold_mode,\n",
    "                           'cooldown': 0,\n",
    "                           'min_lr': self.hparams.min_lr,\n",
    "                           'eps': self.hparams.eps,\n",
    "                           'monitor': self.hparams.monitor,\n",
    "                           'verbose': True\n",
    "                           }\n",
    "            return {\"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler}\n",
    "        \n",
    "         \n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported lr_decay_policy {self.hparams.lr_decay_policy}\")\n",
    "            \n",
    "\n",
    "        return [optimizer], [scheduler]\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 - Test training hyperperameters and framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-12T19:36:33.179387Z",
     "start_time": "2021-05-12T19:36:27.282227Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for view coronal, description unet_single_label_psz064_coronal_evalue1T1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miclab/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/optimizers.py:129: RuntimeWarning: Found unsupported keys in the lr scheduler dict: ['mode', 'factor', 'patience', 'threshold', 'threshold_mode', 'cooldown', 'min_lr', 'eps', 'verbose']\n",
      "  rank_zero_warn(f'Found unsupported keys in the lr scheduler dict: {extra_keys}', RuntimeWarning)\n",
      "\n",
      "  | Name  | Type | Params\n",
      "-------------------------------\n",
      "0 | model | UNet | 7.8 M \n",
      "-------------------------------\n",
      "7.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "7.8 M     Total params\n",
      "31.051    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLATEAU - scheduling_patience_lrepochs =  3  lr_decay_factor =  0.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ccfef414eee447687bf99a51298e78b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Training'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type | Params\n",
      "-------------------------------\n",
      "0 | model | UNet | 7.8 M \n",
      "-------------------------------\n",
      "7.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "7.8 M     Total params\n",
      "31.051    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Disabling sagital flip aug\n",
      "Starting training for view sagittal, description unet_single_label_psz064_sagittal_evalue1T1\n",
      "PLATEAU - scheduling_patience_lrepochs =  3  lr_decay_factor =  0.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0081dc18c70549c5bdbdd163ad7ef663",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Training'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type | Params\n",
      "-------------------------------\n",
      "0 | model | UNet | 7.8 M \n",
      "-------------------------------\n",
      "7.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "7.8 M     Total params\n",
      "31.051    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training for view axial, description unet_single_label_psz064_axial_evalue1T1\n",
      "PLATEAU - scheduling_patience_lrepochs =  3  lr_decay_factor =  0.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2e131df4ed84030a5b1af3f51075d6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Training'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 12.8 s, sys: 2.28 s, total: 15.1 s\n",
      "Wall time: 14.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "original_desc = original_hyperparameters[\"description\"]\n",
    "original_view = original_hyperparameters[\"data_view\"]\n",
    "if original_view == \"all\":\n",
    "    views = [\"coronal\", \"sagittal\", \"axial\"]\n",
    "else:\n",
    "    views = [original_view]\n",
    "    \n",
    "for view in views:\n",
    "    new_desc = original_desc.replace(\"all\", view)\n",
    "    hyperparameters = original_hyperparameters.copy()\n",
    "    \n",
    "    hyperparameters[\"data_view\"] = view\n",
    "    hyperparameters[\"description\"] = new_desc\n",
    "    if view == \"sagittal\":\n",
    "        print(\"Disabling sagital flip aug\")\n",
    "        hyperparameters[\"taug_flip_prob\"] = 0  # disable flip on sagittal\n",
    "    print(f\"Starting training for view {view}, description {new_desc}\")\n",
    "    \n",
    "    model = Segmentor(hparams=hyperparameters)\n",
    "    data = MyDataModule(hparams=hyperparameters)\n",
    "\n",
    "    # Setting the trainer for fast_dev_run\n",
    "    trainer_just_1batch = pl.Trainer(fast_dev_run=True, \n",
    "                                     profiler=None,\n",
    "                                     gpus=1,  # GPU number\n",
    "                                     precision=32,  \n",
    "                                     logger=False,  \n",
    "                                     callbacks=None,  #\n",
    "                                     checkpoint_callback=False,\n",
    "                                     )\n",
    "\n",
    "    trainer_just_1batch.fit(model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 - Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-12T20:27:25.059269Z",
     "start_time": "2021-05-12T19:36:38.085337Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miclab/miniconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:360: UserWarning: Checkpoint directory checkpoints/unet_single_label exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for view coronal, description unet_single_label_psz064_coronal_evalue1T1\n",
      "checkpoints/unet_single_label\n",
      "Hyperparameters:\n",
      "\n",
      "experiment_name: unet_single_label\n",
      "description: unet_single_label_psz064_coronal_evalue1T1\n",
      "data_view: coronal\n",
      "dataset_folder: ./Data/Patches/single_label_psz064/\n",
      "subjects_list: ./training_subjects_randomized.txt\n",
      "in_channels: ['evalue1', 'T1']\n",
      "masks: ['staple_oh']\n",
      "cnn_architecture: unet\n",
      "input_size: 32\n",
      "n_inchannels: 2\n",
      "n_outchannels: 2\n",
      "init_features: 32\n",
      "taug_angle: 5\n",
      "taug_flip_prob: 0.5\n",
      "max_epochs: 2000\n",
      "patience: 20\n",
      "learning_threshold: 0.01\n",
      "batch_size: 256\n",
      "split_train_val: 0.2\n",
      "opt_name: Adam\n",
      "min_lr: 1e-06\n",
      "eps: 1e-05\n",
      "monitor: val_loss\n",
      "lr: 0.001\n",
      "scheduling_patience_lrepochs: 3\n",
      "lr_decay_factor: 0.1\n",
      "lr_decay_policy: plateau\n",
      "lr_decay_mode: min\n",
      "lr_decay_threshold_mode: abs\n",
      "lossweighs: [[0, 1]]\n",
      "func_weights: [1]\n",
      "train_loss_funcs: ['DiceLoss_weighs']\n",
      "train_metric: DiceMetric_weighs\n",
      "train_metricweighs: [0, 1]\n",
      "val_loss_funcs: ['DiceLoss_weighs']\n",
      "val_metric: DiceMetric_weighs\n",
      "val_metricweighs: [0, 1]\n",
      "nworkers: 8\n",
      "val_transform: {}\n",
      "PLATEAU - scheduling_patience_lrepochs =  3  lr_decay_factor =  0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name  | Type | Params\n",
      "-------------------------------\n",
      "0 | model | UNet | 7.8 M \n",
      "-------------------------------\n",
      "7.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "7.8 M     Total params\n",
      "31.051    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4286b4e1f24d412b929c7f41b935ad9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validation sanity check'), FloatProgress(value=1.0, bar_style='info', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b2b4b6848214d3eb37a20af1595d855",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Training'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 0.252\n",
      "Epoch 0, global step 90: val_loss reached 0.25241 (best 0.25241), saving model to \"/home/miclab/Python_codes/tahalmus_benchmark_diffusion_dev-main/code/checkpoints/unet_single_label/unet_single_label_psz064_coronal_evalue1T1_17-12-2021_15-24-epoch=00-val_loss=0.25.ckpt\" as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.165 >= min_delta = 0.0. New best score: 0.087\n",
      "Epoch 1, global step 181: val_loss reached 0.08712 (best 0.08712), saving model to \"/home/miclab/Python_codes/tahalmus_benchmark_diffusion_dev-main/code/checkpoints/unet_single_label/unet_single_label_psz064_coronal_evalue1T1_17-12-2021_15-24-epoch=01-val_loss=0.09.ckpt\" as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.030 >= min_delta = 0.0. New best score: 0.057\n",
      "Epoch 2, global step 272: val_loss reached 0.05714 (best 0.05714), saving model to \"/home/miclab/Python_codes/tahalmus_benchmark_diffusion_dev-main/code/checkpoints/unet_single_label/unet_single_label_psz064_coronal_evalue1T1_17-12-2021_15-24-epoch=02-val_loss=0.06.ckpt\" as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.007 >= min_delta = 0.0. New best score: 0.050\n",
      "Epoch 3, global step 363: val_loss reached 0.05038 (best 0.05038), saving model to \"/home/miclab/Python_codes/tahalmus_benchmark_diffusion_dev-main/code/checkpoints/unet_single_label/unet_single_label_psz064_coronal_evalue1T1_17-12-2021_15-24-epoch=03-val_loss=0.05.ckpt\" as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.004 >= min_delta = 0.0. New best score: 0.046\n",
      "Epoch 4, global step 454: val_loss reached 0.04598 (best 0.04598), saving model to \"/home/miclab/Python_codes/tahalmus_benchmark_diffusion_dev-main/code/checkpoints/unet_single_label/unet_single_label_psz064_coronal_evalue1T1_17-12-2021_15-24-epoch=04-val_loss=0.05.ckpt\" as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.046\n",
      "Epoch 5, global step 545: val_loss reached 0.04587 (best 0.04587), saving model to \"/home/miclab/Python_codes/tahalmus_benchmark_diffusion_dev-main/code/checkpoints/unet_single_label/unet_single_label_psz064_coronal_evalue1T1_17-12-2021_15-24-epoch=05-val_loss=0.05.ckpt\" as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.004 >= min_delta = 0.0. New best score: 0.041\n",
      "Epoch 6, global step 636: val_loss reached 0.04142 (best 0.04142), saving model to \"/home/miclab/Python_codes/tahalmus_benchmark_diffusion_dev-main/code/checkpoints/unet_single_label/unet_single_label_psz064_coronal_evalue1T1_17-12-2021_15-24-epoch=06-val_loss=0.04.ckpt\" as top 1\n",
      "/home/miclab/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:897: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn('Detected KeyboardInterrupt, attempting graceful shutdown...')\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type | Params\n",
      "-------------------------------\n",
      "0 | model | UNet | 7.8 M \n",
      "-------------------------------\n",
      "7.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "7.8 M     Total params\n",
      "31.051    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disabling sagital flip aug\n",
      "Starting training for view sagittal, description unet_single_label_psz064_sagittal_evalue1T1\n",
      "checkpoints/unet_single_label\n",
      "Hyperparameters:\n",
      "\n",
      "experiment_name: unet_single_label\n",
      "description: unet_single_label_psz064_sagittal_evalue1T1\n",
      "data_view: sagittal\n",
      "dataset_folder: ./Data/Patches/single_label_psz064/\n",
      "subjects_list: ./training_subjects_randomized.txt\n",
      "in_channels: ['evalue1', 'T1']\n",
      "masks: ['staple_oh']\n",
      "cnn_architecture: unet\n",
      "input_size: 32\n",
      "n_inchannels: 2\n",
      "n_outchannels: 2\n",
      "init_features: 32\n",
      "taug_angle: 5\n",
      "taug_flip_prob: 0\n",
      "max_epochs: 2000\n",
      "patience: 20\n",
      "learning_threshold: 0.01\n",
      "batch_size: 256\n",
      "split_train_val: 0.2\n",
      "opt_name: Adam\n",
      "min_lr: 1e-06\n",
      "eps: 1e-05\n",
      "monitor: val_loss\n",
      "lr: 0.001\n",
      "scheduling_patience_lrepochs: 3\n",
      "lr_decay_factor: 0.1\n",
      "lr_decay_policy: plateau\n",
      "lr_decay_mode: min\n",
      "lr_decay_threshold_mode: abs\n",
      "lossweighs: [[0, 1]]\n",
      "func_weights: [1]\n",
      "train_loss_funcs: ['DiceLoss_weighs']\n",
      "train_metric: DiceMetric_weighs\n",
      "train_metricweighs: [0, 1]\n",
      "val_loss_funcs: ['DiceLoss_weighs']\n",
      "val_metric: DiceMetric_weighs\n",
      "val_metricweighs: [0, 1]\n",
      "nworkers: 8\n",
      "val_transform: {}\n",
      "PLATEAU - scheduling_patience_lrepochs =  3  lr_decay_factor =  0.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b8d07f0ae134c8183b6e3e8a221e170",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validation sanity check'), FloatProgress(value=1.0, bar_style='info', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a072ccc8e2c40f8976e3ebc508cc6b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Training'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 0.208\n",
      "Epoch 0, global step 90: val_loss reached 0.20770 (best 0.20770), saving model to \"/home/miclab/Python_codes/tahalmus_benchmark_diffusion_dev-main/code/checkpoints/unet_single_label/unet_single_label_psz064_sagittal_evalue1T1_17-12-2021_15-31-epoch=00-val_loss=0.21.ckpt\" as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.135 >= min_delta = 0.0. New best score: 0.073\n",
      "Epoch 1, global step 181: val_loss reached 0.07262 (best 0.07262), saving model to \"/home/miclab/Python_codes/tahalmus_benchmark_diffusion_dev-main/code/checkpoints/unet_single_label/unet_single_label_psz064_sagittal_evalue1T1_17-12-2021_15-31-epoch=01-val_loss=0.07.ckpt\" as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.016 >= min_delta = 0.0. New best score: 0.057\n",
      "Epoch 2, global step 272: val_loss reached 0.05703 (best 0.05703), saving model to \"/home/miclab/Python_codes/tahalmus_benchmark_diffusion_dev-main/code/checkpoints/unet_single_label/unet_single_label_psz064_sagittal_evalue1T1_17-12-2021_15-31-epoch=02-val_loss=0.06.ckpt\" as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.007 >= min_delta = 0.0. New best score: 0.050\n",
      "Epoch 3, global step 363: val_loss reached 0.05022 (best 0.05022), saving model to \"/home/miclab/Python_codes/tahalmus_benchmark_diffusion_dev-main/code/checkpoints/unet_single_label/unet_single_label_psz064_sagittal_evalue1T1_17-12-2021_15-31-epoch=03-val_loss=0.05.ckpt\" as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.005 >= min_delta = 0.0. New best score: 0.045\n",
      "Epoch 4, global step 454: val_loss reached 0.04539 (best 0.04539), saving model to \"/home/miclab/Python_codes/tahalmus_benchmark_diffusion_dev-main/code/checkpoints/unet_single_label/unet_single_label_psz064_sagittal_evalue1T1_17-12-2021_15-31-epoch=04-val_loss=0.05.ckpt\" as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 545: val_loss was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.003 >= min_delta = 0.0. New best score: 0.043\n",
      "Epoch 6, global step 636: val_loss reached 0.04274 (best 0.04274), saving model to \"/home/miclab/Python_codes/tahalmus_benchmark_diffusion_dev-main/code/checkpoints/unet_single_label/unet_single_label_psz064_sagittal_evalue1T1_17-12-2021_15-31-epoch=06-val_loss=0.04.ckpt\" as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.001 >= min_delta = 0.0. New best score: 0.042\n",
      "Epoch 7, global step 727: val_loss reached 0.04164 (best 0.04164), saving model to \"/home/miclab/Python_codes/tahalmus_benchmark_diffusion_dev-main/code/checkpoints/unet_single_label/unet_single_label_psz064_sagittal_evalue1T1_17-12-2021_15-31-epoch=07-val_loss=0.04.ckpt\" as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.001 >= min_delta = 0.0. New best score: 0.041\n",
      "Epoch 8, global step 818: val_loss reached 0.04057 (best 0.04057), saving model to \"/home/miclab/Python_codes/tahalmus_benchmark_diffusion_dev-main/code/checkpoints/unet_single_label/unet_single_label_psz064_sagittal_evalue1T1_17-12-2021_15-31-epoch=08-val_loss=0.04.ckpt\" as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 909: val_loss was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.001 >= min_delta = 0.0. New best score: 0.039\n",
      "Epoch 10, global step 1000: val_loss reached 0.03930 (best 0.03930), saving model to \"/home/miclab/Python_codes/tahalmus_benchmark_diffusion_dev-main/code/checkpoints/unet_single_label/unet_single_label_psz064_sagittal_evalue1T1_17-12-2021_15-31-epoch=10-val_loss=0.04.ckpt\" as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11, global step 1091: val_loss was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12, global step 1182: val_loss was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13, global step 1273: val_loss was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14, global step 1364: val_loss was not in top 1\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type | Params\n",
      "-------------------------------\n",
      "0 | model | UNet | 7.8 M \n",
      "-------------------------------\n",
      "7.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "7.8 M     Total params\n",
      "31.051    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for view axial, description unet_single_label_psz064_axial_evalue1T1\n",
      "checkpoints/unet_single_label\n",
      "Hyperparameters:\n",
      "\n",
      "experiment_name: unet_single_label\n",
      "description: unet_single_label_psz064_axial_evalue1T1\n",
      "data_view: axial\n",
      "dataset_folder: ./Data/Patches/single_label_psz064/\n",
      "subjects_list: ./training_subjects_randomized.txt\n",
      "in_channels: ['evalue1', 'T1']\n",
      "masks: ['staple_oh']\n",
      "cnn_architecture: unet\n",
      "input_size: 32\n",
      "n_inchannels: 2\n",
      "n_outchannels: 2\n",
      "init_features: 32\n",
      "taug_angle: 5\n",
      "taug_flip_prob: 0.5\n",
      "max_epochs: 2000\n",
      "patience: 20\n",
      "learning_threshold: 0.01\n",
      "batch_size: 256\n",
      "split_train_val: 0.2\n",
      "opt_name: Adam\n",
      "min_lr: 1e-06\n",
      "eps: 1e-05\n",
      "monitor: val_loss\n",
      "lr: 0.001\n",
      "scheduling_patience_lrepochs: 3\n",
      "lr_decay_factor: 0.1\n",
      "lr_decay_policy: plateau\n",
      "lr_decay_mode: min\n",
      "lr_decay_threshold_mode: abs\n",
      "lossweighs: [[0, 1]]\n",
      "func_weights: [1]\n",
      "train_loss_funcs: ['DiceLoss_weighs']\n",
      "train_metric: DiceMetric_weighs\n",
      "train_metricweighs: [0, 1]\n",
      "val_loss_funcs: ['DiceLoss_weighs']\n",
      "val_metric: DiceMetric_weighs\n",
      "val_metricweighs: [0, 1]\n",
      "nworkers: 8\n",
      "val_transform: {}\n",
      "PLATEAU - scheduling_patience_lrepochs =  3  lr_decay_factor =  0.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73d9a75c4788486784ff383c8fcf6c46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validation sanity check'), FloatProgress(value=1.0, bar_style='info', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b275f72be6b44b6cb64f07cc66bd4f99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Training'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "DONE!\n",
      "CPU times: user 22min 10s, sys: 26.8 s, total: 22min 37s\n",
      "Wall time: 22min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "       \n",
    "original_desc = original_hyperparameters[\"description\"]\n",
    "original_view = original_hyperparameters[\"data_view\"]\n",
    "if original_view == \"all\":\n",
    "    views = [\"coronal\", \"sagittal\", \"axial\"]\n",
    "else:\n",
    "    views = [original_view]\n",
    "    \n",
    "for view in views:\n",
    "    new_desc = original_desc.replace(\"all\", view)\n",
    "    hyperparameters = original_hyperparameters.copy()\n",
    "    \n",
    "    hyperparameters[\"data_view\"] = view\n",
    "    hyperparameters[\"description\"] = new_desc\n",
    "    if view == \"sagittal\":\n",
    "        print(\"Disabling sagital flip aug\")\n",
    "        hyperparameters[\"taug_flip_prob\"] = 0  # disable flip on sagittal view\n",
    "    print(f\"Starting training for view {view}, description {new_desc}\")\n",
    "    model = Segmentor(hparams=hyperparameters)\n",
    "    data = MyDataModule(hparams=hyperparameters)\n",
    " \n",
    "    #callbacks configuration\n",
    "    prefix = hyperparameters[\"description\"] + '_' + time.strftime(\"%d-%m-%Y_%H-%M\")\n",
    "    ckpt_path = os.path.join(\"checkpoints\", hyperparameters[\"experiment_name\"])\n",
    "    print(ckpt_path)\n",
    "    callbacks = [EarlyStopping(monitor=\"val_loss\",  # logging variable\n",
    "                               patience=hyperparameters['patience'], \n",
    "                               verbose=True, \n",
    "                               mode=hyperparameters['lr_decay_mode'] \n",
    "                               ),\n",
    "                 ModelCheckpoint(dirpath=ckpt_path, \n",
    "                                 filename=prefix + '-{epoch:02d}-{val_loss:.2f}',\n",
    "                                 verbose=True,\n",
    "                                 monitor=\"val_loss\", \n",
    "                                 save_top_k=1,\n",
    "                                 mode=\"min\") ]\n",
    "    trainer_normal = pl.Trainer(max_epochs=hyperparameters[\"max_epochs\"],\n",
    "                                gpus=1,\n",
    "                                precision=32,  \n",
    "                                callbacks=callbacks,  \n",
    "                                checkpoint_callback=True,  \n",
    "                                accumulate_grad_batches=2,  \n",
    "                                resume_from_checkpoint=None,  \n",
    "                                progress_bar_refresh_rate=50  \n",
    "                                                              \n",
    "                                )\n",
    "\n",
    "    print(\"Hyperparameters:\\n\")\n",
    "    for k, v in hyperparameters.items():\n",
    "        print(f'{k}: {v}')\n",
    "\n",
    "    trainer_normal.fit(model, data)\n",
    "\n",
    "print('DONE!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open tensor board monitor\n",
    "%tensorboard --logdir lightning_logs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
